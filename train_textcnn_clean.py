import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from torchtext.vocab import build_vocab_from_iterator
from torch.utils.data import DataLoader
import jieba

from model.textcnn import TextCNN
from utils.dataset_utils import TextDataset

print("ğŸ” [é˜¶æ®µ1] æ­£åœ¨åŠ è½½æ•°æ®...")
df = pd.read_csv("æƒ…æ„Ÿè¯„è®ºæ•°æ®é›†_sample.csv", encoding="utf-8-sig")
df["tokens"] = df["content"].apply(lambda x: [w for w in jieba.cut(x) if w.strip()])

# æ„å»ºè¯æ±‡è¡¨
def yield_tokens(data_iter):
    for tokens in data_iter:
        yield tokens

print("ğŸ” [é˜¶æ®µ2] æ„å»ºè¯è¡¨...")
vocab = build_vocab_from_iterator(yield_tokens(df["tokens"]), specials=["<pad>", "<unk>"])
vocab.set_default_index(vocab["<unk>"])

MAX_LEN = 30
df["input_ids"] = df["tokens"].apply(lambda x: vocab(x)[:MAX_LEN] + [vocab["<pad>"]] * max(0, MAX_LEN - len(x)))

print("ğŸ” [é˜¶æ®µ3] åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†...")
X_train, X_test, y_train, y_test = train_test_split(df["input_ids"].tolist(), df["label"].tolist(), test_size=0.2, random_state=42)
train_dataset = TextDataset(X_train, y_train)
test_dataset = TextDataset(X_test, y_test)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8)

print(f"âœ… è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}, æµ‹è¯•æ ·æœ¬æ•°: {len(test_dataset)}")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = TextCNN(len(vocab), 100, 3).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

print("ğŸš€ [é˜¶æ®µ4] å¼€å§‹è®­ç»ƒæ¨¡å‹...")
for epoch in range(10):
    model.train()
    total_loss = 0
    for X, y in train_loader:
        X, y = X.to(device), y.to(device)
        optimizer.zero_grad()
        out = model(X)
        loss = criterion(out, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}, Loss: {total_loss:.4f}")
print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

print("ğŸ§ª [é˜¶æ®µ5] å¼€å§‹æµ‹è¯•æ¨¡å‹...")
model.eval()
all_preds, all_labels = [], []
with torch.no_grad():
    for X, y in test_loader:
        X = X.to(device)
        out = model(X)
        preds = torch.argmax(out, dim=1).cpu().tolist()
        all_preds.extend(preds)
        all_labels.extend(y.tolist())

print("ğŸ“Š åˆ†ç±»æŠ¥å‘Šï¼š")
print(classification_report(all_labels, all_preds, target_names=["class0", "class1", "class2"]))

print("ğŸ“ˆ æ­£åœ¨ç”Ÿæˆæ··æ·†çŸ©é˜µå›¾...")
cm = confusion_matrix(all_labels, all_preds)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["0", "1", "2"],
            yticklabels=["0", "1", "2"],
            cbar=False)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("TextCNN Confusion Matrix")
plt.tight_layout()
plt.savefig("textcnn_confusion_matrix.png")
plt.show()
